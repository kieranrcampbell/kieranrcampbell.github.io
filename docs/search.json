[
  {
    "objectID": "posts/notes-comp-bio/index.html",
    "href": "posts/notes-comp-bio/index.html",
    "title": "What makes a good computational genomics method?",
    "section": "",
    "text": "Start with a problem, not a solution\nA seemingly common strategy when starting a new computational genomics methods problem is to\n\nThink of cool new model\nImplement the cool new model\nWork out where the cool new model might be useful\n\nI have \\(n=1\\) concrete evidence for this but suspect it’s very common. For example, when single cell RNA-sequencing methods were created, there was a rush to develop specific methods for differential expression (comparing if the expression of gene differs between two conditions) rather than relying on existing methods for bulk data. However, later benchmarking papers hinted that bulk differential expression tools performed best on average. In other words, make sure there is a problem to be solved rather than creating a solution first or imagining there is a problem without confirming. A really good place to start with this is via benchmarking/testing existing methods and working out where they fail/perform poorly. It’s in those gaps the potential for important new methods lies.\n\n\nSeek a new perspective, not an improved score\nA related issue is to look an existing method, think\n\nthis doesn’t achieve 100% accuracy\n\n(however you choose to define accuracy), then think\n\nI bet I can come up with a better method\n\nThere are some merits to this approach. For example, in the field of computer vision circa 2012, the top-1 accuracy on ImageNet using hand-engineered features was ~50%. The next year, along came deep learning and raised this to ~65%, a relative increase of ~30% which is mindblowing1:\n\n\n\nTop-1 accuracy on ImageNet 2011-2021. Source: paperswithcode.com\n\n\nThe catch is, this is happens rarely in applied genomics research. It’s much more likely that after extensive methods development and (hyper-)parameter tuning, we see a 2-5% increase2 in whatever score is used to benchmark the method.\nThe issue here is: is any new biology discovered as a result of improving a score by 2-5%? I suspect the answer is frequently “no”, and a lot of frustration of wet-lab biologists with the computational methods community is chasing such benchmarks rather than trying to uncover new biology / insights from the data. I also suspect that 30% bump in accuracy on ImageNet has laid the foundation for thinking\n\nIf only I could create the right (deep learning) method, I’ll solve this!\n\nfrom many researchers (including myself), which in reality it’s uncommon for this to throw the doors open to new insights.\nSo what’s the alternative? The alternative is to take a fresh perspective of the data in some that discovers something new, which is hard. One nice example of this work on RNA velocity, which took existing single-cell RNA-seq data, but realized that by quantifying spliced vs. unspliced read counts with a new model (the fresh perspective) and in doing so could predict3 the future state of cells (the something new). But the fact remains that such new approaches are hard and consequently rare, but a good angle may be to go first really deep on the biology then try to align that with what’s possible from the data.\n\n\nHave an evaluation-first approach\nAfter successfully identifying a problem, the next important yet still hard question is how do you know when you’ve solved the problem? In a supervised learning setup this is somewhat straightforward: a successful method will improve the accuracy on held out test data4.\nHowever, in computational genomics it’s often hard to apply such a concept, since (i) labelled (ground truth data) may be expensive to obtain and/or non existent, and (ii) many of the problems are unsupervised (e.g. clustering or dimensionality reduction), so there is no such thing as a “ground truth”.\nGiven this, how do we say if one method is “better” than another? Some common approaches are\n\nDevelop a set of heuristics that reflect some quality of the ground truth. For example, maybe a “good” clustering is one where the cluster markers reflect known cell lineages. We’ve developed some methodology around Bayesian optimization in such settings previously.\nGet ground truth labels from somewhere (anywhere), or at least labels that may correlate with ground truth\nAssess methods on their robustness to perturbations in the data / model hyperparameters / etc.\n\nThe crucial consideration isn’t the exact evaluation strategy used but to have one in place before you start. It’s much better to have\n\n\n\n\nflowchart LR\n  A[Identify problem] --> B[Consider\\nevaluation\\nstrategy]\n  B --> C[Realize no\\nstrategy exists]\n  C --> D[Abandon\\nproject]\n\n\n\n\n\n\n\n\nthan\n\n\n\n\nflowchart LR\n  A[Identify problem] --> B[Develop new\\n method]\n  B --> C[Consider\\nevaluation\\nstrategy]\n  C --> D[Realize no\\nstrategy exists]\n  C --> E[Realize new\\nmethod is inferior\\nto existing]\n  D & E --> F[Abandon\\nproject]\n\n\n\n\n\n\n\n\n\n\nBalance what’s important with what’s possible\nThe last consideration is probably the most obvious: aim for projects that are both important and feasible.\nWhat is important? Projects that will deliver some new biological insight, unblock some impediment to progress in a sub-field, or (most importantly of all) deliver something back to the clinic that will improve patient outcomes.\nWhat is feasible? Projects where the data to answer the question exist, to which you have access, and/or for which you have collaborators able to validate any predictions you make.\nThe problem is, the intersection of the two is often small:\n\n\n\nAim for the intersection of what’s important and what’s feasible.\n\n\nThis often turns into a chicken and egg problem: you’ve identified \\(X\\) as a really crucial issue in the field that needs solved urgently, but you only have the data/collaborators/skills to solve \\(Y\\). Consequently, iteration is needed to refine the project until you’ve identified a related problem that is still worthwhile to solve but actually feasible. This may take the form of limiting project scope (“let’s do a pan-cancer analysis” becomes “let’s do the analysis in the cancer type we have data for”) or settling on predicting a surrogate of what you actually want (e.g. predicting treatment response becomes predicting known treatment response signatures). What’s important is to consider this up-front and not get stuck on the not feasible / not important trap5.\n\n\n\n\n\nFootnotes\n\n\nThis is a simplified re-telling but true to a first approximation.↩︎\nThis is an educated guess, but I suspect after we account for over-fitting this acts as an upper bound.↩︎\nTo a first approximation↩︎\nThough other parts remain not straightforward.↩︎\nWhat’s important is also highly subjective and not always obvious until (many decades) after the fact.↩︎"
  },
  {
    "objectID": "posts/bayesian-linear-regression/index.html",
    "href": "posts/bayesian-linear-regression/index.html",
    "title": "Gibbs sampling for Bayesian linear regression in Python",
    "section": "",
    "text": "Forsaking both, I’ve written a brief guide about how to implement Gibbs sampling for Bayesian linear regression in Python. This comes out of some more complex work we’re doing with factor analysis, but the basic ideas for deriving a Gibbs sampler are the same. Introductory tutorials to Gibbs sampling seem to be fairly scarce, and while Radford Neal briefly covers it in his lectures here I go into a little more detail in the derivations below. If you find any mistakes or if anything is unclear, please get in touch: kieranc [at] well.ox.ac.uk.\n\nBayesian linear regression\nHere we are interested in Gibbs sampling for normal linear regression with one independent variable. We assume we have paired data \\((y_i, x_i) , i = 1, \\ldots, N\\). We wish to find the posterior distributions of the coefficients \\(\\beta_0\\) (the intercept), \\(\\beta_1\\) (the gradient) and of the precision \\(\\tau\\), which is the reciprocal of the variance. The model can be written as\n\\[ y_i \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_i, 1 / \\tau) \\]\nor equivalently\n\\[ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon, \\; \\; \\epsilon \\sim \\mathcal{N}(0, 1 / \\tau) \\]\nThe likelihood for this model may be written as the product over \\[ N \\] iid observations\n\\[ L(y_1, \\ldots, y_N, x_1, \\ldots, x_N | \\beta_0, \\beta_1, \\tau) = \\prod_{i = 1}^N \\mathcal{N}(\\beta_0 + \\beta_1 x_i, 1 / \\tau) \\]\nWe also wish to place conjugate priors on \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\tau\\) for reasons that will become apparent later. For these we choose\n\\[ \\beta_0 \\sim \\mathcal{N}(\\mu_0, 1 / \\tau_0) \\]\n\\[ \\beta_1 \\sim \\mathcal{N}(\\mu_1, 1 / \\tau_1) \\]\n\\[ \\tau \\sim \\text{Gamma}(\\alpha, \\beta) \\]\n\n\nGibbs sampling\nGibbs sampling works as follows: suppose we have two parameters \\(\\theta_1\\) and \\(\\theta_2\\) and some data \\(x\\). Our goal is to find the posterior distribution of \\(p(\\theta_1, \\theta_2 | x)\\). To do this in a Gibbs sampling regime we need to work out the conditional distributions \\(p(\\theta_1 | \\theta_2, x)\\) and \\(p(\\theta_2 | \\theta_1, x)\\) (which is typically the hard part). The Gibbs updates are then\n\nPick some initial \\(\\theta_2^{(i)}\\).\nSample \\(\\theta_1^{(i+1)} \\sim p(\\theta_1 | \\theta_2^{(i)}, x)\\)\nSample \\(\\theta_2^{(i+1)} \\sim p(\\theta_2 | \\theta_1^{(i+1)}, x)\\)\n\nThen increment \\(i\\) and repeat \\(K\\) times to draw \\(K\\) samples. This is equivalent to sampling new values for a given variable while holding all others constant. The key thing to remember in Gibbs sampling is to always use the most recent parameter values for all samples (e.g. sample \\(\\theta_2^{(i+1)} \\sim p(\\theta_2 | \\theta_1^{(i+1)}, x)\\) and not \\(\\theta_2^{(i+1)} \\sim p(\\theta_2 | \\theta_1^{(i)}, x)\\) provided \\(\\theta_1^{(i+1)}\\) has already been sampled).\nThe massive advantage of Gibbs sampling over other MCMC methods (namely Metropolis-Hastings) is that no tuning parameters are required! The downside is the need of a fair bit of maths to derive the updates, which even then aren’t always guaranteed to exist.\n\n\nPythonic setup\nFirst let’s set ourselves up with python imports and functions so we can implement the functions as we derive them.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n\n\nDeriving a Gibbs sampler\nThe general approach to deriving an update for a variable is\n\nWrite down the posterior conditional density in log-form\nThrow away all terms that don’t depend on the current sampling variable\nPretend this is the density for your variable of interest and all other variables are fixed. What distribution does the log-density remind you of?\nThat’s your conditional sampling density!\n\nWe go through this for our three variables step by step below.\n\nUpdates for \\(\\beta_0\\)\nWe’re interested in finding\n\\[p(\\beta_0 | \\beta_1, \\tau, y, x) \\propto p(y, x | \\beta_0, \\beta_1, \\tau) p(\\beta_0)\\]\nNote that \\(p(y, x | \\beta_0, \\beta_1, \\tau)\\) is just the likelihood from above and \\(p(\\beta_0)\\) is simply \\(\\mathcal{N}(\\mu_0, 1 / \\tau_0)\\).\nIf a variable \\(x\\) follows a normal distribution with mean \\(\\mu\\) and precision \\(\\tau\\) then the log-dependence on \\(x\\) is \\(-\\frac{\\tau}{2}(x - \\mu)^2 \\propto -\\frac{\\tau}{2} x^2 + \\tau \\mu x\\). So if we can force the log-posterior conditional density into a quadratic form then the coefficient of \\(x^2\\) (where \\(x\\) is the variable of interest) will be \\(\\tau \\mu\\) and the coefficient of \\(x^2\\) will be \\(-\\frac{\\tau}{2}\\).\nHence the log-dependence on \\(\\beta_0\\) is\n\\[ -\\frac{\\tau_0}{2}(\\beta_0 - \\mu_0)^2 - \\frac{\\tau}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 \\]\nAlthough it’s perhaps not obvious, this expression is quadratic in \\(\\beta_0\\), meaning the conditional sampling density for \\(\\beta_0\\) will also be normal. A bit of algebra (dropping all terms that don’t involve \\(\\beta_0\\) ) takes us to\n\\[ -\\frac{\\tau_0}{2} \\beta_0^2 +\\tau_0 \\mu_0 \\beta_0 -\\frac{\\tau}{2} N \\beta_0^2\n+ \\tau \\sum_i (y_i - \\beta_1 x_i) \\beta_0\\]\nIn other words the coefficient of \\(\\beta_0\\) is \\(\\tau_0 \\mu_0 + \\tau \\sum_i (y_i - \\beta_1 x_i)\\) while the coefficient of \\(\\beta_0^2\\) is \\(-\\frac{\\tau_0}{2} -\\frac{\\tau}{2} N\\). This implies the conditional sampling distribution of \\(\\beta_0\\) is\n\\[ \\beta_0 | \\beta_1, \\tau, \\tau_0, \\mu_0, x, y \\sim \\mathcal{N}\\left( \\frac{\\tau_0 \\mu_0 + \\tau \\sum_i (y_i - \\beta_1 x_i)}{\\tau_0 + \\tau N}, 1 / (\\tau_0 + \\tau N) \\right) \\]\nLet’s turn that into a python function:\n\ndef sample_beta_0(y, x, beta_1, tau, mu_0, tau_0):\n    N = len(y)\n    assert len(x) == N\n    precision = tau_0 + tau * N\n    mean = tau_0 * mu_0 + tau * np.sum(y - beta_1 * x)\n    mean /= precision\n    return np.random.normal(mean, 1 / np.sqrt(precision))\n\nSweet! Now back to the maths.\n\n\nUpdate for \\(\\beta_1\\)\nSimilarly to \\(\\beta_0\\), the dependence of the conditional log-posterior is given by\n\\[ -\\frac{\\tau_1}{2}(\\beta_1 - \\mu_1)^2 - \\frac{\\tau}{2} \\sum_{i=1}^N (y_i - \\beta_0 - \\beta_1 x_i)^2 \\]\nwhich if we expand out and drop all terms that don’t include \\(\\beta_1\\) we get\n\\[ -\\frac{\\tau_1}{2} \\beta_1^2 +\\tau_1 \\mu_1 \\beta_1 -\\frac{\\tau}{2} \\sum_i x_i^2 \\beta_1^2\n+ \\tau \\sum_i (y_i - \\beta_0) x_i \\beta_1\\]\nso the coefficient of \\(\\beta_1\\) is \\(\\tau_1 \\mu_1 + \\tau \\sum_i (y_i - \\beta_0) x_i\\) while the coefficient of \\(\\beta_1^2\\) is \\(-\\frac{\\tau_1}{2} -\\frac{\\tau}{2} \\sum_i x_i^2\\). Therefore the conditional sampling density of \\(\\beta_1\\) is\n\\[ \\beta_1 | \\beta_0, \\tau, \\mu_1, \\tau_1, x, y \\sim \\mathcal{N}\\left( \\frac{\\tau_1 \\mu_1 + \\tau  \\sum_i (y_i - \\beta_0) x_i}{\\tau_1 + \\tau \\sum_i x_i^2}, 1 / (\\tau_1 + \\tau \\sum_i x_i^2) \\right) \\]\nLet’s turn that into a Python function too:\n\ndef sample_beta_1(y, x, beta_0, tau, mu_1, tau_1):\n    N = len(y)\n    assert len(x) == N\n    precision = tau_1 + tau * np.sum(x * x)\n    mean = tau_1 * mu_1 + tau * np.sum( (y - beta_0) * x)\n    mean /= precision\n    return np.random.normal(mean, 1 / np.sqrt(precision))\n\n\n\nUpdate for \\(\\tau\\)\nDeriving the Gibbs update for \\(\\tau\\) is the trickiest part of this exercise as we have to deal with non-Gaussian distributions. First let’s introduce the Gamma distribution, parametrised by \\(\\alpha\\) and \\(\\beta\\). Up to the normalising constant the probability of an observation \\(x\\) under a Gamma density is given by \\(p(x; \\alpha, \\beta) \\propto \\beta^\\alpha x^{\\alpha - 1} e^{-\\beta x}\\) and so the log-dependency of any terms involving \\(x\\) is given by\n\\[ l(x; \\alpha, \\beta) \\propto (\\alpha - 1) \\log x - \\beta x \\]\nNow back to our derivation. We want\n\\[ p(\\tau | \\beta_0, \\beta_1, y, x) \\propto p(y, x | \\beta_0 \\beta_1, \\tau) p(\\tau) \\]\nwhich in this case is a density of\n\\[ \\prod_{i = 1}^N \\mathcal{N}(y_i | \\beta_0 + \\beta_1 x_i, 1 / \\tau) \\times \\text{Gamma}(\\tau | \\alpha, \\beta) \\]\nThe key question to ask here is, what’s the density of \\(\\tau\\) assuming all other parameters are held constant? If we look at the log density of this expression we get\n\\[ \\frac{N}{2} \\log \\tau - \\frac{\\tau}{2} \\sum_i (y_i - \\beta_0 - \\beta_1 x_i)^2 + (\\alpha - 1) \\log \\tau - \\beta \\tau \\]\nwhich has a coefficient of \\(\\tau\\) of \\(-\\sum\\_i \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2} - \\beta\\) and a coefficient of \\(\\log \\tau\\) of \\(\\frac{N}{2} + \\alpha - 1\\). If you look at the equation of the log-density of the Gamma distribution above, this implies that \\(\\tau\\) as a conditional sampling density of\n\\[ \\tau | \\beta_0, \\beta_1, \\alpha, \\beta, x, y \\sim \\text{Gamma} \\left( \\alpha + \\frac{N}{2}, \\beta + \\sum_i \\frac{(y_i - \\beta_0 - \\beta_1 x_i)^2}{2} \\right) \\]\nWe can now code this into python. np.random.gamma uses the shape and scale parameterisation of a Gamma distribution, where the shape \\(k = \\alpha\\) but the scale \\(\\theta = 1 / \\beta\\), so we need to invert our expression for \\(\\beta\\) before sampling:\n\ndef sample_tau(y, x, beta_0, beta_1, alpha, beta):\n    N = len(y)\n    alpha_new = alpha + N / 2\n    resid = y - beta_0 - beta_1 * x\n    beta_new = beta + np.sum(resid * resid) / 2\n    return np.random.gamma(alpha_new, 1 / beta_new)\n\n\n\nSome synthetic data\nTo test our Gibbs sampler we’ll need some synthetic data. Let’s keep things simple - set \\(\\beta_0 = -1\\), \\(\\beta_1 = 2\\) and \\(\\tau = 1\\):\n\nbeta_0_true = -1\nbeta_1_true = 2\ntau_true = 1\n\nN = 50\nx = np.random.uniform(low = 0, high = 4, size = N)\ny = np.random.normal(beta_0_true + beta_1_true * x, 1 / np.sqrt(tau_true))\n\nsynth_plot = plt.plot(x, y, \"o\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\n\n\n\n\n\n\nWriting our Gibbs sampler\nNow we’re ready to write the Gibbs sampler. Apart from the data we need to supply initial parameter estimates and hyper parameters. We can place \\(\\mathcal{N}(0, 1)\\) priors on \\(\\beta_0\\) and \\(\\beta_1\\) and a \\(\\text{Gamma}(2,1)\\) prior on \\(\\tau\\). It then makes sense to initialise the sampler at the maximum likeihood estimates of the priors.\n\n## specify initial values\ninit = {\"beta_0\": 0,\n        \"beta_1\": 0,\n        \"tau\": 2}\n\n## specify hyper parameters\nhypers = {\"mu_0\": 0,\n         \"tau_0\": 1,\n         \"mu_1\": 0,\n         \"tau_1\": 1,\n         \"alpha\": 2,\n         \"beta\": 1}\n\nWe’re then ready to code up our Gibbs sampler, which simply follows the sequence of sampling statements as explained above.\n\ndef gibbs(y, x, iters, init, hypers):\n    assert len(y) == len(x)\n    beta_0 = init[\"beta_0\"]\n    beta_1 = init[\"beta_1\"]\n    tau = init[\"tau\"]\n    \n    trace = np.zeros((iters, 3)) ## trace to store values of beta_0, beta_1, tau\n    \n    for it in range(iters):\n        beta_0 = sample_beta_0(y, x, beta_1, tau, hypers[\"mu_0\"], hypers[\"tau_0\"])\n        beta_1 = sample_beta_1(y, x, beta_0, tau, hypers[\"mu_1\"], hypers[\"tau_1\"])\n        tau = sample_tau(y, x, beta_0, beta_1, hypers[\"alpha\"], hypers[\"beta\"])\n        trace[it,:] = np.array((beta_0, beta_1, tau))\n        \n    trace = pd.DataFrame(trace)\n    trace.columns = ['beta_0', 'beta_1', 'tau']\n        \n    return trace\n\nLet’s test it out. We can use the synthetic data, initialisation and hyper-parameters defined above and run for 1000 iterations.\n\niters = 1000\ntrace = gibbs(y, x, iters, init, hypers)\n\nWe can then plot the traces for the three variables, which is simply the values of the variables against the iteration. Inspecting trace plots for convergence is a bit of a dark art in MCMC inferences. Over the first few (or in cases of Metropolis-Hastings, many) iterations you expect the values to change quite significantly. Then they should reach some equilibrium distribution which will be the posterior distribution of that variable. Let’s have a look for our variables:\n\ntraceplot = trace.plot()\ntraceplot.set_xlabel(\"Iteration\")\ntraceplot.set_ylabel(\"Parameter value\")\n\n\n\n\nWe can see that over the first 20 or so iterations the values change significantly before going to some constant value of around \\(\\beta_0 = -1\\), \\(\\beta_1 = 2\\) and \\(\\tau = 1\\), which are the true values from the synthetic data. Even if it’s obvious that the variables converge early it is convention to define a ‘burn-in’ period where we assume the parameters are still converging, which is typically half of the iterations. Therefore, we can define a new DataFrame that contains the final 500 iterations called trace_burnt, and plot histograms of the values:\n\ntrace_burnt = trace[500:999]\nprint(trace_burnt.head())\n\n       beta_0    beta_1       tau\n500 -1.428293  2.374115  0.971644\n501 -1.409478  2.270175  0.910554\n502 -1.531690  2.447890  1.450373\n503 -1.732455  2.450335  1.076261\n504 -1.674809  2.417664  0.941923\n\n\n\n# hist_plot = trace_burnt.hist(bins = 30)\n# # hist_plot\nsns.histplot(trace_burnt, bins=100)\n\n\n\n\nFinally, we can report the posterior mean and standard deviations of the parameters and check they’re consistent with the ‘true’ ones we defined earlier:\n\nprint(trace_burnt.mean())\n\nbeta_0   -1.159797\nbeta_1    2.180474\ntau       1.134067\ndtype: float64\n\nprint(trace_burnt.std())\n\nbeta_0    0.258064\nbeta_1    0.117294\ntau       0.245233\ndtype: float64\n\n\nWe see that the posterior means always fall within at most one standard deviation of the true value.\nAnd there we have it, a Gibbs sampler for Bayesian linear regression in Python. There are many topics we haven’t covered here, such as thinning observations in MCMC runs or alternative model specifications such as Automatic Relevance Determination (ARD) priors. There are also many more interesting topics in Gibbs sampling such as blocked and collapsed Gibbs samplers, an introduction to which can be found in the wikipedia article."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "I am a researcher in Toronto, Canada interesting in statistical modelling machine learning for single-cell and cancer genomics. I get up to the following things:\n\nI lead a research group at the Lunenfeld-Tanenbaum Research Institute\nWe’re interested in building probabilistic models to analyze data from new technologies like single-cell RNA-sequencing and highly multiplexed imaging, particularly to understand tumour-immune interactions\nI am an Assistant Professor at the Departments of Molecular Genetics, Statistical Sciences, and Computer Science at the University of Toronto and teach the Foundational Computational Biology course\nI hold affiliate faculty positions with the Vector Institute and Ontario Institute for Cancer Research\nMost of the software we write is on the group and my personal Github\n\n\n\n\n\n\n\n\n\n\n\n5 min\n\n\n\nJun 1, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n10 min\n\n\n\nMay 10, 2016\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog posts",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 1, 2023\n\n\nWhat makes a good computational genomics method?\n\n\nKieran R Campbell\n\n\n\n\nMay 10, 2016\n\n\nGibbs sampling for Bayesian linear regression in Python\n\n\nKieran R Campbell\n\n\n\n\n\n\nNo matching items"
  }
]