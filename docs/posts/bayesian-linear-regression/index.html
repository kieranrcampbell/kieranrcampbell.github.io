<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Kieran R Campbell">
<meta name="dcterms.date" content="2016-05-10">

<title>Kieran R Campbell - Gibbs sampling for Bayesian linear regression in Python</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-D1X92L00M8"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-D1X92L00M8', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Kieran R Campbell</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">About</a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../posts.html">Blog posts</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/kieranrcampbell"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/kieranrcampbell"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Gibbs sampling for Bayesian linear regression in Python</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">statistics</div>
                <div class="quarto-category">bayesian</div>
                <div class="quarto-category">python</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Kieran R Campbell </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 10, 2016</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>If you do any work in Bayesian statistics, you’ll know you spend a lot of time hanging around waiting for MCMC samplers to run. The question is then what do you spend that time doing? Maybe you’ve read <a href="https://medium.com/@zip/how-i-conquered-procrastination-8347bc458648#.ngfmvxeke">every</a> <a href="https://medium.com/life-learning/the-single-biggest-reason-most-people-procrastinate-in-life-d286e543d475#.cu9kpnu5z">single</a> <a href="https://medium.com/@james_clear/how-to-stop-procrastinating-by-using-the-2-minute-rule-9f271b3ecf56#.u30n5h7wp">article</a> <a href="https://betterhumans.coach.me/how-to-overcome-procrastination-b8118060298d#.esrjg3dcz">on</a> <a href="https://medium.com/life-tips/7-surprising-ways-procrastination-can-boost-productivity-1b5d923d639c#.p47xbb747">Medium</a> <a href="https://medium.com/@larrykim/15-procrastination-beating-techniques-to-boost-productivity-infographic-258e2ccb4a1a#.moxvpn4vx">about</a> <a href="https://medium.com/life-learning/10-ways-i-deal-with-my-own-procrastination-1822e3d0c5ae#.ipcmr4vvu">avoiding</a> <a href="https://medium.com/@namzo/procrastination-d31c98aa054a#.yklxsrr0b">procrastination</a> or you’re worried that <a href="http://i.imgur.com/CACCr8I.gifv">those cute dog gifs</a> are using up too much CPU power.</p>
<p>Forsaking both, I’ve written a brief guide about how to implement Gibbs sampling for Bayesian linear regression in Python. This comes out of some more complex work we’re doing with factor analysis, but the basic ideas for deriving a Gibbs sampler are the same. Introductory tutorials to Gibbs sampling seem to be fairly scarce, and while Radford Neal briefly covers it in his lectures <a href="http://www.cs.toronto.edu/~radford/csc2541.S11/week3.pdf">here</a> I go into a little more detail in the derivations below. If you find any mistakes or if anything is unclear, please get in touch: kieranc [at] well.ox.ac.uk.</p>
<section id="bayesian-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="bayesian-linear-regression">Bayesian linear regression</h3>
<p>Here we are interested in Gibbs sampling for normal linear regression with one independent variable. We assume we have paired data <span class="math inline">\((y_i, x_i) , i = 1, \ldots, N\)</span>. We wish to find the posterior distributions of the coefficients <span class="math inline">\(\beta_0\)</span> (the intercept), <span class="math inline">\(\beta_1\)</span> (the gradient) and of the precision <span class="math inline">\(\tau\)</span>, which is the reciprocal of the variance. The model can be written as</p>
<p><span class="math display">\[ y_i \sim \mathcal{N}(\beta_0 + \beta_1 x_i, 1 / \tau) \]</span></p>
<p>or equivalently</p>
<p><span class="math display">\[ y_i = \beta_0 + \beta_1 x_i + \epsilon, \; \; \epsilon \sim \mathcal{N}(0, 1 / \tau) \]</span></p>
<p>The likelihood for this model may be written as the product over <span class="math display">\[ N \]</span> iid observations</p>
<p><span class="math display">\[ L(y_1, \ldots, y_N, x_1, \ldots, x_N | \beta_0, \beta_1, \tau) = \prod_{i = 1}^N \mathcal{N}(\beta_0 + \beta_1 x_i, 1 / \tau) \]</span></p>
<p>We also wish to place <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate priors</a> on <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\tau\)</span> for reasons that will become apparent later. For these we choose</p>
<p><span class="math display">\[ \beta_0 \sim \mathcal{N}(\mu_0, 1 / \tau_0) \]</span></p>
<p><span class="math display">\[ \beta_1 \sim \mathcal{N}(\mu_1, 1 / \tau_1) \]</span></p>
<p><span class="math display">\[ \tau \sim \text{Gamma}(\alpha, \beta) \]</span></p>
</section>
<section id="gibbs-sampling" class="level3">
<h3 class="anchored" data-anchor-id="gibbs-sampling">Gibbs sampling</h3>
<p><a href="https://en.wikipedia.org/wiki/Gibbs_sampling">Gibbs sampling</a> works as follows: suppose we have two parameters <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span> and some data <span class="math inline">\(x\)</span>. Our goal is to find the posterior distribution of <span class="math inline">\(p(\theta_1, \theta_2 | x)\)</span>. To do this in a Gibbs sampling regime we need to work out the conditional distributions <span class="math inline">\(p(\theta_1 | \theta_2, x)\)</span> and <span class="math inline">\(p(\theta_2 | \theta_1, x)\)</span> (which is typically the hard part). The Gibbs updates are then</p>
<ol type="1">
<li>Pick some initial <span class="math inline">\(\theta_2^{(i)}\)</span>.</li>
<li>Sample <span class="math inline">\(\theta_1^{(i+1)} \sim p(\theta_1 | \theta_2^{(i)}, x)\)</span></li>
<li>Sample <span class="math inline">\(\theta_2^{(i+1)} \sim p(\theta_2 | \theta_1^{(i+1)}, x)\)</span></li>
</ol>
<p>Then increment <span class="math inline">\(i\)</span> and repeat <span class="math inline">\(K\)</span> times to draw <span class="math inline">\(K\)</span> samples. This is equivalent to sampling new values for a given variable <em>while holding all others constant</em>. The key thing to remember in Gibbs sampling is to always use the most recent parameter values for all samples (e.g.&nbsp;sample <span class="math inline">\(\theta_2^{(i+1)} \sim p(\theta_2 | \theta_1^{(i+1)}, x)\)</span> and not <span class="math inline">\(\theta_2^{(i+1)} \sim p(\theta_2 | \theta_1^{(i)}, x)\)</span> provided <span class="math inline">\(\theta_1^{(i+1)}\)</span> has already been sampled).</p>
<p>The massive advantage of Gibbs sampling over other MCMC methods (namely Metropolis-Hastings) is that no tuning parameters are required! The downside is the need of a fair bit of maths to derive the updates, which even then aren’t always guaranteed to exist.</p>
</section>
<section id="pythonic-setup" class="level3">
<h3 class="anchored" data-anchor-id="pythonic-setup">Pythonic setup</h3>
<p>First let’s set ourselves up with python imports and functions so we can implement the functions as we derive them.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="deriving-a-gibbs-sampler" class="level3">
<h3 class="anchored" data-anchor-id="deriving-a-gibbs-sampler">Deriving a Gibbs sampler</h3>
<p>The general approach to deriving an update for a variable is</p>
<ol type="1">
<li>Write down the posterior conditional density in log-form</li>
<li>Throw away all terms that don’t depend on the current sampling variable</li>
<li>Pretend this is the density for your variable of interest and all other variables are fixed. What distribution does the log-density remind you of?</li>
<li>That’s your conditional sampling density!</li>
</ol>
<p>We go through this for our three variables step by step below.</p>
<section id="updates-for-beta_0" class="level4">
<h4 class="anchored" data-anchor-id="updates-for-beta_0">Updates for <span class="math inline">\(\beta_0\)</span></h4>
<p>We’re interested in finding</p>
<p><span class="math display">\[p(\beta_0 | \beta_1, \tau, y, x) \propto p(y, x | \beta_0, \beta_1, \tau) p(\beta_0)\]</span></p>
<p>Note that <span class="math inline">\(p(y, x | \beta_0, \beta_1, \tau)\)</span> is just the likelihood from above and <span class="math inline">\(p(\beta_0)\)</span> is simply <span class="math inline">\(\mathcal{N}(\mu_0, 1 / \tau_0)\)</span>.</p>
<p>If a variable <span class="math inline">\(x\)</span> follows a normal distribution with mean <span class="math inline">\(\mu\)</span> and precision <span class="math inline">\(\tau\)</span> then the log-dependence on <span class="math inline">\(x\)</span> is <span class="math inline">\(-\frac{\tau}{2}(x - \mu)^2 \propto -\frac{\tau}{2} x^2 + \tau \mu x\)</span>. So if we can force the log-posterior conditional density into a quadratic form then the coefficient of <span class="math inline">\(x^2\)</span> (where <span class="math inline">\(x\)</span> is the variable of interest) will be <span class="math inline">\(\tau \mu\)</span> and the coefficient of <span class="math inline">\(x^2\)</span> will be <span class="math inline">\(-\frac{\tau}{2}\)</span>.</p>
<p>Hence the log-dependence on <span class="math inline">\(\beta_0\)</span> is</p>
<p><span class="math display">\[ -\frac{\tau_0}{2}(\beta_0 - \mu_0)^2 - \frac{\tau}{2} \sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_i)^2 \]</span></p>
<p>Although it’s perhaps not obvious, this expression is quadratic in <span class="math inline">\(\beta_0\)</span>, meaning the conditional sampling density for <span class="math inline">\(\beta_0\)</span> will also be normal. A bit of algebra (dropping all terms that don’t involve <span class="math inline">\(\beta_0\)</span> ) takes us to</p>
<p><span class="math display">\[ -\frac{\tau_0}{2} \beta_0^2 +\tau_0 \mu_0 \beta_0 -\frac{\tau}{2} N \beta_0^2
+ \tau \sum_i (y_i - \beta_1 x_i) \beta_0\]</span></p>
<p>In other words the coefficient of <span class="math inline">\(\beta_0\)</span> is <span class="math inline">\(\tau_0 \mu_0 + \tau \sum_i (y_i - \beta_1 x_i)\)</span> while the coefficient of <span class="math inline">\(\beta_0^2\)</span> is <span class="math inline">\(-\frac{\tau_0}{2} -\frac{\tau}{2} N\)</span>. This implies the conditional sampling distribution of <span class="math inline">\(\beta_0\)</span> is</p>
<p><span class="math display">\[ \beta_0 | \beta_1, \tau, \tau_0, \mu_0, x, y \sim \mathcal{N}\left( \frac{\tau_0 \mu_0 + \tau \sum_i (y_i - \beta_1 x_i)}{\tau_0 + \tau N}, 1 / (\tau_0 + \tau N) \right) \]</span></p>
<p>Let’s turn that into a python function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_beta_0(y, x, beta_1, tau, mu_0, tau_0):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(x) <span class="op">==</span> N</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> tau_0 <span class="op">+</span> tau <span class="op">*</span> N</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> tau_0 <span class="op">*</span> mu_0 <span class="op">+</span> tau <span class="op">*</span> np.<span class="bu">sum</span>(y <span class="op">-</span> beta_1 <span class="op">*</span> x)</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> precision</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(mean, <span class="dv">1</span> <span class="op">/</span> np.sqrt(precision))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Sweet! Now back to the maths.</p>
</section>
<section id="update-for-beta_1" class="level4">
<h4 class="anchored" data-anchor-id="update-for-beta_1">Update for <span class="math inline">\(\beta_1\)</span></h4>
<p>Similarly to <span class="math inline">\(\beta_0\)</span>, the dependence of the conditional log-posterior is given by</p>
<p><span class="math display">\[ -\frac{\tau_1}{2}(\beta_1 - \mu_1)^2 - \frac{\tau}{2} \sum_{i=1}^N (y_i - \beta_0 - \beta_1 x_i)^2 \]</span></p>
<p>which if we expand out and drop all terms that don’t include <span class="math inline">\(\beta_1\)</span> we get</p>
<p><span class="math display">\[ -\frac{\tau_1}{2} \beta_1^2 +\tau_1 \mu_1 \beta_1 -\frac{\tau}{2} \sum_i x_i^2 \beta_1^2
+ \tau \sum_i (y_i - \beta_0) x_i \beta_1\]</span></p>
<p>so the coefficient of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\tau_1 \mu_1 + \tau \sum_i (y_i - \beta_0) x_i\)</span> while the coefficient of <span class="math inline">\(\beta_1^2\)</span> is <span class="math inline">\(-\frac{\tau_1}{2} -\frac{\tau}{2} \sum_i x_i^2\)</span>. Therefore the conditional sampling density of <span class="math inline">\(\beta_1\)</span> is</p>
<p><span class="math display">\[ \beta_1 | \beta_0, \tau, \mu_1, \tau_1, x, y \sim \mathcal{N}\left( \frac{\tau_1 \mu_1 + \tau  \sum_i (y_i - \beta_0) x_i}{\tau_1 + \tau \sum_i x_i^2}, 1 / (\tau_1 + \tau \sum_i x_i^2) \right) \]</span></p>
<p>Let’s turn that into a Python function too:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_beta_1(y, x, beta_0, tau, mu_1, tau_1):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(x) <span class="op">==</span> N</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> tau_1 <span class="op">+</span> tau <span class="op">*</span> np.<span class="bu">sum</span>(x <span class="op">*</span> x)</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">=</span> tau_1 <span class="op">*</span> mu_1 <span class="op">+</span> tau <span class="op">*</span> np.<span class="bu">sum</span>( (y <span class="op">-</span> beta_0) <span class="op">*</span> x)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    mean <span class="op">/=</span> precision</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.normal(mean, <span class="dv">1</span> <span class="op">/</span> np.sqrt(precision))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="update-for-tau" class="level4">
<h4 class="anchored" data-anchor-id="update-for-tau">Update for <span class="math inline">\(\tau\)</span></h4>
<p>Deriving the Gibbs update for <span class="math inline">\(\tau\)</span> is the trickiest part of this exercise as we have to deal with non-Gaussian distributions. First let’s introduce the Gamma distribution, parametrised by <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Up to the normalising constant the probability of an observation <span class="math inline">\(x\)</span> under a Gamma density is given by <span class="math inline">\(p(x; \alpha, \beta) \propto \beta^\alpha x^{\alpha - 1} e^{-\beta x}\)</span> and so the log-dependency of any terms involving <span class="math inline">\(x\)</span> is given by</p>
<p><span class="math display">\[ l(x; \alpha, \beta) \propto (\alpha - 1) \log x - \beta x \]</span></p>
<p>Now back to our derivation. We want</p>
<p><span class="math display">\[ p(\tau | \beta_0, \beta_1, y, x) \propto p(y, x | \beta_0 \beta_1, \tau) p(\tau) \]</span></p>
<p>which in this case is a density of</p>
<p><span class="math display">\[ \prod_{i = 1}^N \mathcal{N}(y_i | \beta_0 + \beta_1 x_i, 1 / \tau) \times \text{Gamma}(\tau | \alpha, \beta) \]</span></p>
<p>The key question to ask here is, <em>what’s the density of</em> <span class="math inline">\(\tau\)</span> assuming all other parameters are held constant? If we look at the log density of this expression we get</p>
<p><span class="math display">\[ \frac{N}{2} \log \tau - \frac{\tau}{2} \sum_i (y_i - \beta_0 - \beta_1 x_i)^2 + (\alpha - 1) \log \tau - \beta \tau \]</span></p>
<p>which has a coefficient of <span class="math inline">\(\tau\)</span> of <span class="math inline">\(-\sum\_i \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2} - \beta\)</span> and a coefficient of <span class="math inline">\(\log \tau\)</span> of <span class="math inline">\(\frac{N}{2} + \alpha - 1\)</span>. If you look at the equation of the log-density of the Gamma distribution above, this implies that <span class="math inline">\(\tau\)</span> as a conditional sampling density of</p>
<p><span class="math display">\[ \tau | \beta_0, \beta_1, \alpha, \beta, x, y \sim \text{Gamma} \left( \alpha + \frac{N}{2}, \beta + \sum_i \frac{(y_i - \beta_0 - \beta_1 x_i)^2}{2} \right) \]</span></p>
<p>We can now code this into python. <code>np.random.gamma</code> uses the shape and scale parameterisation of a Gamma distribution, where the shape <span class="math inline">\(k = \alpha\)</span> but the scale <span class="math inline">\(\theta = 1 / \beta\)</span>, so we need to invert our expression for <span class="math inline">\(\beta\)</span> before sampling:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sample_tau(y, x, beta_0, beta_1, alpha, beta):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    N <span class="op">=</span> <span class="bu">len</span>(y)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    alpha_new <span class="op">=</span> alpha <span class="op">+</span> N <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    resid <span class="op">=</span> y <span class="op">-</span> beta_0 <span class="op">-</span> beta_1 <span class="op">*</span> x</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    beta_new <span class="op">=</span> beta <span class="op">+</span> np.<span class="bu">sum</span>(resid <span class="op">*</span> resid) <span class="op">/</span> <span class="dv">2</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.gamma(alpha_new, <span class="dv">1</span> <span class="op">/</span> beta_new)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="some-synthetic-data" class="level4">
<h4 class="anchored" data-anchor-id="some-synthetic-data">Some synthetic data</h4>
<p>To test our Gibbs sampler we’ll need some synthetic data. Let’s keep things simple - set <span class="math inline">\(\beta_0 = -1\)</span>, <span class="math inline">\(\beta_1 = 2\)</span> and <span class="math inline">\(\tau = 1\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>beta_0_true <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>beta_1_true <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>tau_true <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>N <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.random.uniform(low <span class="op">=</span> <span class="dv">0</span>, high <span class="op">=</span> <span class="dv">4</span>, size <span class="op">=</span> N)</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.random.normal(beta_0_true <span class="op">+</span> beta_1_true <span class="op">*</span> x, <span class="dv">1</span> <span class="op">/</span> np.sqrt(tau_true))</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>synth_plot <span class="op">=</span> plt.plot(x, y, <span class="st">"o"</span>)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"x"</span>)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"y"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
</section>
<section id="writing-our-gibbs-sampler" class="level3">
<h3 class="anchored" data-anchor-id="writing-our-gibbs-sampler">Writing our Gibbs sampler</h3>
<p>Now we’re ready to write the Gibbs sampler. Apart from the data we need to supply initial parameter estimates and hyper parameters. We can place <span class="math inline">\(\mathcal{N}(0, 1)\)</span> priors on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and a <span class="math inline">\(\text{Gamma}(2,1)\)</span> prior on <span class="math inline">\(\tau\)</span>. It then makes sense to initialise the sampler at the maximum likeihood estimates of the priors.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">## specify initial values</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>init <span class="op">=</span> {<span class="st">"beta_0"</span>: <span class="dv">0</span>,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">"beta_1"</span>: <span class="dv">0</span>,</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"tau"</span>: <span class="dv">2</span>}</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">## specify hyper parameters</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>hypers <span class="op">=</span> {<span class="st">"mu_0"</span>: <span class="dv">0</span>,</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>         <span class="st">"tau_0"</span>: <span class="dv">1</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>         <span class="st">"mu_1"</span>: <span class="dv">0</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>         <span class="st">"tau_1"</span>: <span class="dv">1</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>         <span class="st">"alpha"</span>: <span class="dv">2</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>         <span class="st">"beta"</span>: <span class="dv">1</span>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We’re then ready to code up our Gibbs sampler, which simply follows the sequence of sampling statements as explained above.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gibbs(y, x, iters, init, hypers):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="bu">len</span>(y) <span class="op">==</span> <span class="bu">len</span>(x)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    beta_0 <span class="op">=</span> init[<span class="st">"beta_0"</span>]</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    beta_1 <span class="op">=</span> init[<span class="st">"beta_1"</span>]</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    tau <span class="op">=</span> init[<span class="st">"tau"</span>]</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> np.zeros((iters, <span class="dv">3</span>)) <span class="co">## trace to store values of beta_0, beta_1, tau</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> it <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        beta_0 <span class="op">=</span> sample_beta_0(y, x, beta_1, tau, hypers[<span class="st">"mu_0"</span>], hypers[<span class="st">"tau_0"</span>])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        beta_1 <span class="op">=</span> sample_beta_1(y, x, beta_0, tau, hypers[<span class="st">"mu_1"</span>], hypers[<span class="st">"tau_1"</span>])</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        tau <span class="op">=</span> sample_tau(y, x, beta_0, beta_1, hypers[<span class="st">"alpha"</span>], hypers[<span class="st">"beta"</span>])</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        trace[it,:] <span class="op">=</span> np.array((beta_0, beta_1, tau))</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    trace <span class="op">=</span> pd.DataFrame(trace)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    trace.columns <span class="op">=</span> [<span class="st">'beta_0'</span>, <span class="st">'beta_1'</span>, <span class="st">'tau'</span>]</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> trace</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Let’s test it out. We can use the synthetic data, initialisation and hyper-parameters defined above and run for 1000 iterations.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>iters <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>trace <span class="op">=</span> gibbs(y, x, iters, init, hypers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can then plot the traces for the three variables, which is simply the values of the variables against the iteration. Inspecting trace plots for convergence is a bit of a dark art in MCMC inferences. Over the first few (or in cases of Metropolis-Hastings, many) iterations you expect the values to change quite significantly. Then they should reach some equilibrium distribution which will be the posterior distribution of that variable. Let’s have a look for our variables:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>traceplot <span class="op">=</span> trace.plot()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>traceplot.set_xlabel(<span class="st">"Iteration"</span>)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>traceplot.set_ylabel(<span class="st">"Parameter value"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-10-3.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>We can see that over the first 20 or so iterations the values change significantly before going to some constant value of around <span class="math inline">\(\beta_0 = -1\)</span>, <span class="math inline">\(\beta_1 = 2\)</span> and <span class="math inline">\(\tau = 1\)</span>, which are the true values from the synthetic data. Even if it’s obvious that the variables converge early it is convention to define a ‘burn-in’ period where we assume the parameters are still converging, which is typically half of the iterations. Therefore, we can define a new <code>DataFrame</code> that contains the final 500 iterations called <code>trace_burnt</code>, and plot histograms of the values:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>trace_burnt <span class="op">=</span> trace[<span class="dv">500</span>:<span class="dv">999</span>]</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(trace_burnt.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       beta_0    beta_1       tau
500 -1.428293  2.374115  0.971644
501 -1.409478  2.270175  0.910554
502 -1.531690  2.447890  1.450373
503 -1.732455  2.450335  1.076261
504 -1.674809  2.417664  0.941923</code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># hist_plot = trace_burnt.hist(bins = 30)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co"># # hist_plot</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>sns.histplot(trace_burnt, bins<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="index_files/figure-html/unnamed-chunk-12-5.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Finally, we can report the posterior mean and standard deviations of the parameters and check they’re consistent with the ‘true’ ones we defined earlier:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(trace_burnt.mean())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta_0   -1.159797
beta_1    2.180474
tau       1.134067
dtype: float64</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(trace_burnt.std())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>beta_0    0.258064
beta_1    0.117294
tau       0.245233
dtype: float64</code></pre>
</div>
</div>
<p>We see that the posterior means always fall within at most one standard deviation of the true value.</p>
<p>And there we have it, a Gibbs sampler for Bayesian linear regression in Python. There are many topics we haven’t covered here, such as thinning observations in MCMC runs or alternative model specifications such as Automatic Relevance Determination (ARD) priors. There are also many more interesting topics in Gibbs sampling such as blocked and collapsed Gibbs samplers, an introduction to which can be found in the <a href="http://en.wikipedia.org/wiki/Gibbs_sampling">wikipedia article</a>.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>